{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Nested.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1rGAhpXfTWtJYzF86fWIHp6VL25Yttpdn","authorship_tag":"ABX9TyP2IRktUHTqEChrtDlwwLHg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"JKwdLxHpXzMH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611138927563,"user_tz":-480,"elapsed":4744,"user":{"displayName":"hao ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOspQRqQvSKFKaSjazc9OUXKV7SHzuh9ZbzbYv=s64","userId":"06578137870287261288"}},"outputId":"1d81be7a-0838-411f-cfbb-4f8dc3a4eb9d"},"source":["# 在google co-lab中，切换到正确工作目录\n","import os\n","if \"Nested.ipynb\" not in os.listdir():\n","  try:\n","    os.chdir(\"drive/MyDrive\")\n","    os.chdir(\"Nested\")\n","  except:\n","    print(\"无法找到正确工作目录！\")\n","\n","# 安装依赖项\n","!sudo pip install -r requirements.txt | grep -v 'Requirement already satisfied'\n","!nvidia-smi\n","\n","# 载入所有需要的包\n","import copy, time, sys\n","from datetime import datetime\n","from random import shuffle\n","from collections import defaultdict\n","import numpy as np\n","import pickle\n","from typing import Optional, Tuple, List, Dict\n","import torch\n","import torch.nn\n","import torch.cuda\n","from torch import Tensor\n","\n","from model.sequence_labeling import BiRecurrentConvCRF4NestedNER\n","from training.logger import get_logger\n","from training.utils import adjust_learning_rate, clip_model_grad, create_opt, pack_target, unpack_prediction, Optimizer\n","from util.evaluate import evaluate\n","from util.utils import Alphabet, save_dynamic_config, load_dynamic_config\n","from reader.reader import Reader"],"execution_count":1,"outputs":[{"output_type":"stream","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3_O20RfbPA7K","executionInfo":{"status":"ok","timestamp":1611138927564,"user_tz":-480,"elapsed":4737,"user":{"displayName":"hao ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOspQRqQvSKFKaSjazc9OUXKV7SHzuh9ZbzbYv=s64","userId":"06578137870287261288"}}},"source":["# 设定全局随机种子，保证可复现性\r\n","def set_random_seed(seed):\r\n","    np.random.seed(seed)\r\n","    torch.manual_seed(seed)\r\n","    torch.cuda.manual_seed(seed)\r\n","    torch.backends.cudnn.deterministic = True\r\n","    torch.backends.cudnn.benchmark = False\r\n","    os.environ['PYTHONHASHSEED'] = str(seed)\r\n","\r\n","set_random_seed(2021)\r\n","\r\n","\r\n","# 全局配置文件设置\r\n","class Config:\r\n","    def __init__(self,dataset) -> None:\r\n","        self.root_path: str = \".\"\r\n","\r\n","        # for data loader\r\n","        self.data_set: str = dataset\r\n","        self.lowercase: Optional[bool] = None\r\n","        self.batch_size: int = 32\r\n","        self.if_shuffle: bool = True\r\n","\r\n","        # override when loading data\r\n","        self.voc_iv_size: Optional[int] = None\r\n","        self.voc_ooev_size: Optional[int] = None\r\n","        self.char_size: Optional[int] = None\r\n","        self.label_size: Optional[int] = None\r\n","\r\n","        # embed size\r\n","        self.token_embed: Optional[int] = None\r\n","        self.char_embed: int = 128\r\n","        self.word_dropout: float = 0.05\r\n","        self.char_dropout: float = 0.00\r\n","\r\n","        # for cnn\r\n","        self.num_filters: int = 256\r\n","        self.kernel_size: int = 3\r\n","\r\n","        # for lstm\r\n","        self.hidden_size: int = 256\r\n","        self.layers: int = 2\r\n","        self.lstm_dropout: float = 0.20\r\n","\r\n","        # for training\r\n","        self.embed_path: str = self.root_path + \"/data/word_vec_{}.pkl\".format(self.data_set)\r\n","\r\n","        self.epoch: int = 150 # TODO: 根据需要设置训练轮数  \r\n","        self.if_gpu: bool = True\r\n","        self.if_gpu = self.if_gpu and torch.cuda.is_available()\r\n","\r\n","        self.opt: Optimizer = Optimizer.AdaBound\r\n","        self.lr: float = 0.001 if self.opt != Optimizer.SGD else 0.1\r\n","        self.final_lr: float = 0.1 if self.opt == Optimizer.AdaBound else None\r\n","        self.l2: float = 0.\r\n","        self.check_every: int = 1\r\n","        self.clip_norm: int = 5\r\n","\r\n","        # for early stop\r\n","        self.lr_patience: int = 3 if self.opt != Optimizer.SGD else 5\r\n","\r\n","        self.data_path: str = self.root_path + \"/data/{}\".format(self.data_set)\r\n","        self.train_data_path: str = self.data_path + \"_train.pkl\"\r\n","        self.dev_data_path: str = self.data_path + \"_dev.pkl\"\r\n","        self.test_data_path: str = self.data_path + \"_test.pkl\"\r\n","        self.idx_data_path: str = self.data_path + \"_idx.pkl\"\r\n","        self.config_data_path: str = self.data_path + \"_config.pkl\"\r\n","        self.model_root_path: str = self.root_path + \"/dumps\"\r\n","        self.model_path: str = self.model_root_path + \"/{}_model\".format(self.data_set)\r\n","\r\n","    def __repr__(self) -> str:\r\n","        return str(vars(self))\r\n","\r\n","config = Config('NLPHW')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"xsee4233iNc_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611139179432,"user_tz":-480,"elapsed":256600,"user":{"displayName":"hao ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOspQRqQvSKFKaSjazc9OUXKV7SHzuh9ZbzbYv=s64","userId":"06578137870287261288"}},"outputId":"c2788aee-c350-498e-bc17-5f80d8112782"},"source":["# gen_data.py\n","def batch_stat(batches: Tuple[List[List[List[int]]],\n","                List[List[List[int]]],\n","                List[List[List[List[int]]]],\n","                List[List[List[Tuple[int, int, int]]]],\n","                List[List[List[bool]]]]) -> None:\n","    all_num = 0\n","    start_num = 0\n","    end_num = 0\n","    for token_iv_batch, token_ooev_batch, char_batch, label_batch, mask_batch in zip(*batches):\n","        for labels in label_batch:\n","            start_dic = defaultdict(list)\n","            end_dic = defaultdict(list)\n","            for ent in labels:\n","                start_dic[(ent[0], ent[2])].append(ent)\n","                end_dic[(ent[1], ent[2])].append(ent)\n","                all_num += 1\n","            for k, v in start_dic.items():\n","                if len(v) > 1:\n","                    start_num += len(v)\n","            for k, v in end_dic.items():\n","                if len(v) > 1:\n","                    end_num += len(v)\n","\n","    print(\"All {}, start {}, end {}\".format(all_num, start_num, end_num))\n","\n","\n","reader = Reader()\n","reader.read_and_gen_vectors_pubmed_word2vec(config.embed_path)\n","reader.read_all_data(\"./data/\" + config.data_set + '/', \"train.txt\", \"dev.txt\", \"test.txt\")\n","\n","(train_batches, dev_batches, test_batches), index = reader.to_batch(config.batch_size)\n","with open(config.train_data_path, 'wb') as f:\n","  pickle.dump(train_batches, f)\n","\n","with open(config.dev_data_path, 'wb') as f:\n","  pickle.dump(dev_batches, f)\n","\n","with open(config.test_data_path, 'wb') as f:\n","  pickle.dump(test_batches, f)\n","\n","with open(config.idx_data_path, 'wb') as f:\n","  pickle.dump(index, f)\n","\n","batch_stat(train_batches)\n","batch_stat(dev_batches)\n","batch_stat(test_batches)\n","\n","# misc config\n","misc_dict = save_dynamic_config(reader)\n","with open(config.config_data_path, 'wb') as f:\n","  pickle.dump(misc_dict, f)\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Max length: 20\n","Threshold 6: 835\n","Max length: 20\n","Threshold 6: 76\n","Max length: 0\n","Threshold 6: 0\n","# mentions: 47006\n","# mentions: 4469\n","# mentions: 0\n","All 47006, start 2149, end 1892\n","All 4469, start 186, end 230\n","All 0, start 0, end 0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vxIZGkvLmdun","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611140000522,"user_tz":-480,"elapsed":1077682,"user":{"displayName":"hao ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOspQRqQvSKFKaSjazc9OUXKV7SHzuh9ZbzbYv=s64","userId":"06578137870287261288"}},"outputId":"247ef754-1c4c-4203-8097-8ab9a972978a"},"source":["# train.py\n","def get_f1(model: BiRecurrentConvCRF4NestedNER, mode: str, file_path: str = None) -> float:\n","    output_res = [None]*1855*2\n","    total_i = 0\n","    with torch.no_grad():\n","        model.eval()\n","\n","        pred_all, pred, recall_all, recall = 0, 0, 0, 0\n","        gold_cross_num = 0\n","        pred_cross_num = 0\n","        if mode == 'dev':\n","            batch_zip = zip(dev_token_iv_batches,\n","                            dev_token_ooev_batches,\n","                            dev_char_batches,\n","                            dev_label_batches,\n","                            dev_mask_batches)\n","        elif mode == 'test':\n","            batch_zip = zip(test_token_iv_batches,\n","                            test_token_ooev_batches,\n","                            test_char_batches,\n","                            test_label_batches,\n","                            test_mask_batches)\n","        else:\n","            raise ValueError\n","\n","        f = None\n","        if file_path is not None:\n","            f = open(file_path, 'w', encoding='utf-8')\n","\n","        for token_iv_batch, token_ooev_batch, char_batch, label_batch, mask_batch in batch_zip:\n","            token_iv_batch_var = torch.LongTensor(np.array(token_iv_batch,dtype=int))\n","            token_ooev_batch_var = torch.LongTensor(np.array(token_ooev_batch))\n","            char_batch_var = torch.LongTensor(np.array(char_batch))\n","            mask_batch_var = torch.ByteTensor(np.array(mask_batch, dtype=np.uint8))\n","            if config.if_gpu:\n","                token_iv_batch_var = token_iv_batch_var.cuda()\n","                token_ooev_batch_var = token_ooev_batch_var.cuda()\n","                char_batch_var = char_batch_var.cuda()\n","                mask_batch_var = mask_batch_var.cuda()\n","\n","            pred_sequence_entities = model.predict(token_iv_batch_var,\n","                                token_ooev_batch_var,\n","                                char_batch_var,\n","                                mask_batch_var)\n","            pred_entities = unpack_prediction(model, pred_sequence_entities)\n","            p_a, p, r_a, r = evaluate(label_batch, pred_entities)\n","\n","            gold_cross_num += 0\n","            pred_cross_num += 0\n","\n","            pred_all += p_a\n","            pred += p\n","            recall_all += r_a\n","            recall += r\n","\n","            if file_path is not None:\n","                for token_iv, token_ooev, mask, label, preds \\\n","                        in zip(token_iv_batch, token_ooev_batch, mask_batch, label_batch, pred_entities):\n","                    words = []\n","                    for t_iv, t_ooev, m in zip(token_iv, token_ooev, mask):\n","                        if not m:\n","                            break\n","                        if t_iv > 0:\n","                            words.append(voc_iv_dict.get_instance(t_iv))\n","                        else:\n","                            words.append(voc_ooev_dict.get_instance(t_ooev))\n","                    index = test_index[total_i] \n","                    total_i += 1\n","                    word_ = ' '.join(words) + '\\n'\n","                    output_res[index*2] = word_\n","                    labels = []\n","                    for p in sorted(preds, key=lambda x: (x[0], x[1], x[2])):\n","                        labels.append(\"{},{} {}\".format(p[0], p[1], label_dict.get_instance(p[2])))\n","                    label_ = '|'.join(labels) + '\\n'\n","                    output_res[index*2+1] = label_\n","        \n","        if file_path is not None:\n","            for i in range(0, len(output_res), 2):\n","                if output_res[i] is not None:\n","                    f.write(output_res[i]) \n","                else:\n","                    f.write('should be\\n')\n","                if output_res[i+1] is not None:\n","                    f.write(output_res[i+1]) \n","                else:\n","                    f.write('\\n')\n","            f.close()\n","\n","        pred = pred / pred_all if pred_all > 0 else 1.\n","        recall = recall / recall_all if recall_all > 0 else 1.\n","        f1 = 2 / ((1. / pred) + (1. / recall)) if pred > 0. and recall > 0. else 0.\n","        logger.info(\"{} precision: {:.2f}%, recall: {:.2f}%, F1: {:.2f}%\".format(mode, pred * 100., recall * 100., f1 * 100.))\n","        return f1\n","\n","\n","# prepare log file\n","serial_number = datetime.now().strftime('%y%m%d_%H%M%S')\n","log_file_path = config.model_path + \"_\" + serial_number + '.tmp'\n","if not os.path.isdir(config.model_root_path):\n","    os.makedirs(config.model_root_path, mode=0o755, exist_ok=True)\n","logger = get_logger('Nested Mention', file=log_file_path)\n","\n","\n","# load data\n","with open(config.train_data_path, 'rb') as f:\n","  train_token_iv_batches, train_token_ooev_batches, train_char_batches, train_label_batches, train_mask_batches = pickle.load(f)\n","with open(config.dev_data_path, 'rb') as f:\n","  dev_token_iv_batches, dev_token_ooev_batches, dev_char_batches, dev_label_batches, dev_mask_batches = pickle.load(f)\n","with open(config.test_data_path, 'rb') as f:\n","  test_token_iv_batches, test_token_ooev_batches, test_char_batches, test_label_batches, test_mask_batches = pickle.load(f)\n","with open(config.idx_data_path, 'rb') as f:\n","  test_index = pickle.load(f)\n","\n","\n","# misc info\n","misc_config: Dict[str, Alphabet] = pickle.load(open(config.config_data_path, 'rb'))\n","voc_iv_dict, voc_ooev_dict, char_dict, label_dict = load_dynamic_config(misc_config)\n","config.voc_iv_size = voc_iv_dict.size()\n","config.voc_ooev_size = voc_ooev_dict.size()\n","config.char_size = char_dict.size()\n","config.label_size = label_dict.size()\n","\n","with open(config.embed_path, 'rb') as f:\n","    vectors: List[np.ndarray] = pickle.load(f)\n","    config.token_embed = vectors[0].size\n","    embedd_word: Tensor = Tensor(vectors)\n","\n","logger.info(config)  # print training setting\n","\n","ner_model = BiRecurrentConvCRF4NestedNER(config.token_embed, config.voc_iv_size, config.voc_ooev_size,\n","                                         config.char_embed, config.char_size, config.num_filters, config.kernel_size,\n","                                         config.label_size, embedd_word,\n","                                         hidden_size=config.hidden_size, layers=config.layers,\n","                                         word_dropout=config.word_dropout, char_dropout=config.char_dropout,\n","                                         lstm_dropout=config.lstm_dropout)\n","if config.if_gpu:\n","    ner_model = ner_model.cuda()\n","\n","parameters = filter(lambda p: p.requires_grad, ner_model.parameters())\n","optimizer, lr_scheduler = create_opt(parameters, config.opt, lr=config.lr, l2=config.l2, lr_patience=config.lr_patience)\n","\n","train_sequence_label_batches = [pack_target(ner_model, train_label_batch, train_mask_batch)\n","                                for train_label_batch, train_mask_batch in zip(train_label_batches, train_mask_batches)]\n","\n","logger.info(\"{} batches expected for training\".format(len(train_token_iv_batches)))\n","logger.info(\"\")\n","best_model = None\n","best_per = float('-inf')\n","best_loss = float('inf')\n","train_all_batches = list(zip(train_token_iv_batches,\n","                             train_token_ooev_batches,\n","                             train_char_batches,\n","                             train_sequence_label_batches,\n","                             train_mask_batches))\n","\n","train_start_time = time.time()\n","num_batches = len(train_all_batches)\n","for e_ in range(1, config.epoch + 1):\n","    logger.info(\"Epoch {:d} (learning rate={:.4f}):\".format(e_, optimizer.param_groups[0]['lr']))\n","    train_err = 0.\n","    train_total = 0.\n","\n","    if config.if_shuffle:\n","        shuffle(train_all_batches)\n","    batch_counter = 0\n","    start_time = time.time()\n","    ner_model.train()\n","    num_back = 0\n","    for token_iv_batch, token_ooev_batch, char_batch, label_batch, mask_batch in train_all_batches:\n","        batch_len = len(token_iv_batch)\n","\n","        token_iv_batch_var = torch.LongTensor(np.array(token_iv_batch))\n","        token_ooev_batch_var = torch.LongTensor(np.array(token_ooev_batch))\n","        char_batch_var = torch.LongTensor(np.array(char_batch))\n","        mask_batch_var = torch.ByteTensor(np.array(mask_batch, dtype=np.uint8))\n","        if config.if_gpu:\n","            token_iv_batch_var = token_iv_batch_var.cuda()\n","            token_ooev_batch_var = token_ooev_batch_var.cuda()\n","            char_batch_var = char_batch_var.cuda()\n","            mask_batch_var = mask_batch_var.cuda()\n","\n","        optimizer.zero_grad()\n","        loss = ner_model.forward(token_iv_batch_var, token_ooev_batch_var, char_batch_var,\n","                                 label_batch, mask_batch_var)\n","        loss.backward()\n","        clip_model_grad(ner_model, config.clip_norm)\n","\n","        batch_counter += 1\n","\n","        optimizer.step(None)\n","\n","        with torch.no_grad():\n","            train_err += loss * batch_len\n","            train_total += batch_len\n","\n","        # update log\n","        if batch_counter % 10 == 0:\n","            time_ave = (time.time() - start_time) / batch_counter\n","            time_left = (num_batches - batch_counter) * time_ave\n","\n","            sys.stdout.write('\\b' * num_back)\n","            sys.stdout.write(' ' * num_back)\n","            sys.stdout.write('\\b' * num_back)\n","            log_info = \"train: {:d}/{:d} loss: {:.4f}, time left (estimated): {:.2f}s\" \\\n","                       .format(batch_counter, num_batches, train_err / train_total, time_left)\n","            sys.stdout.write(log_info)\n","            sys.stdout.flush()\n","            num_back = len(log_info)\n","\n","    sys.stdout.write('\\b' * num_back)\n","    sys.stdout.write(' ' * num_back)\n","    sys.stdout.write('\\b' * num_back)\n","    logger.info(\"train: {:d} loss: {:.4f}, time: {:.2f}s\"\n","                .format(num_batches, train_err / train_total, time.time() - start_time))\n","\n","    if e_ % config.check_every != 0:\n","        continue\n","\n","    # evaluating dev and always save the best\n","    cur_time = time.time()\n","    f1 = get_f1(ner_model, 'dev')\n","    logger.info(\"dev step took {:.4f} seconds\".format(time.time() - cur_time))\n","    logger.info(\"\")\n","\n","    # early stop\n","    if f1 > best_per:\n","        best_per = f1\n","        del best_model\n","        best_model = copy.deepcopy(ner_model)\n","    if train_err < best_loss:\n","        best_loss = train_err\n","    if not adjust_learning_rate(lr_scheduler, e_, train_err, f1):\n","        break\n","\n","logger.info(\"training step took {:.4f} seconds\".format(time.time() - train_start_time))\n","logger.info(\"best dev F1: {:.2f}%\".format(best_per * 100.))\n","logger.info(\"\")\n","\n","serial_number = datetime.now().strftime('%y%m%d_%H%M%S')\n","this_model_path = config.model_path + \"_\" + serial_number\n","if not os.path.isdir(config.model_root_path):\n","    os.makedirs(config.model_root_path, mode=0o755, exist_ok=True)\n","\n","# remember to eval after loading the model. for the reason of batchnorm and dropout\n","cur_time = time.time()\n","f1 = get_f1(best_model, 'test', file_path=this_model_path + '.result.txt')\n","logger.info(\"test step took {:.4f} seconds\".format(time.time() - cur_time))\n","\n","logger.info(\"Dumping model to {}\".format(this_model_path + '.pt'))\n","torch.save(best_model.state_dict(), this_model_path + '.pt')\n","\n","os.rename(log_file_path, this_model_path + '.log.txt')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["2021-01-20 10:40:19,315 - Nested Mention - INFO - {'root_path': '.', 'data_set': 'NLPHW', 'lowercase': None, 'batch_size': 32, 'if_shuffle': True, 'voc_iv_size': 2231687, 'voc_ooev_size': 1647, 'char_size': 85, 'label_size': 5, 'token_embed': 200, 'char_embed': 128, 'word_dropout': 0.05, 'char_dropout': 0.0, 'num_filters': 256, 'kernel_size': 3, 'hidden_size': 256, 'layers': 2, 'lstm_dropout': 0.2, 'embed_path': './data/word_vec_NLPHW.pkl', 'epoch': 1, 'if_gpu': False, 'opt': <Optimizer.AdaBound: 'AdaBound'>, 'lr': 0.001, 'final_lr': 0.1, 'l2': 0.0, 'check_every': 1, 'clip_norm': 5, 'lr_patience': 3, 'data_path': './data/NLPHW', 'train_data_path': './data/NLPHW_train.pkl', 'dev_data_path': './data/NLPHW_dev.pkl', 'test_data_path': './data/NLPHW_test.pkl', 'idx_data_path': './data/NLPHW_idx.pkl', 'config_data_path': './data/NLPHW_config.pkl', 'model_root_path': './dumps', 'model_path': './dumps/NLPHW_model'}\n","2021-01-20 10:40:24,350 - Nested Mention - INFO - 470 batches expected for training\n","2021-01-20 10:40:24,352 - Nested Mention - INFO - \n","2021-01-20 10:40:24,359 - Nested Mention - INFO - Epoch 1 (learning rate=0.0010):\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/adabound/adabound.py:94: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n","  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"],"name":"stderr"},{"output_type":"stream","text":["train: 10/470 loss: 51.8946, time left (estimated): 607.66s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 20/470 loss: 35.0792, time left (estimated): 595.50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 30/470 loss: 29.3548, time left (estimated): 615.26s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 40/470 loss: 26.1153, time left (estimated): 596.52s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 50/470 loss: 24.7462, time left (estimated): 616.62s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 60/470 loss: 24.0041, time left (estimated): 617.81s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 70/470 loss: 23.2585, time left (estimated): 613.77s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 80/470 loss: 22.2160, time left (estimated): 587.93s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 90/470 loss: 21.4954, time left (estimated): 566.06s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 100/470 loss: 20.8041, time left (estimated): 544.94s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 110/470 loss: 20.3127, time left (estimated): 528.15s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 120/470 loss: 20.1333, time left (estimated): 517.63s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 130/470 loss: 19.7884, time left (estimated): 503.18s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 140/470 loss: 19.6933, time left (estimated): 501.08s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 150/470 loss: 19.1622, time left (estimated): 481.01s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 160/470 loss: 18.9447, time left (estimated): 480.97s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 170/470 loss: 18.4822, time left (estimated): 464.28s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 180/470 loss: 18.0625, time left (estimated): 447.19s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 190/470 loss: 17.7567, time left (estimated): 431.97s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 200/470 loss: 17.4294, time left (estimated): 418.71s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 210/470 loss: 17.1947, time left (estimated): 406.57s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 220/470 loss: 17.0126, time left (estimated): 393.52s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 230/470 loss: 16.7095, time left (estimated): 375.95s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 240/470 loss: 16.4835, time left (estimated): 359.72s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 250/470 loss: 16.2536, time left (estimated): 343.39s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 260/470 loss: 16.0045, time left (estimated): 326.29s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 270/470 loss: 15.8352, time left (estimated): 311.53s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 280/470 loss: 15.6352, time left (estimated): 295.62s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 290/470 loss: 15.5734, time left (estimated): 288.45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 300/470 loss: 15.3787, time left (estimated): 271.18s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 310/470 loss: 15.1512, time left (estimated): 253.15s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 320/470 loss: 15.0325, time left (estimated): 237.68s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 330/470 loss: 14.8557, time left (estimated): 221.04s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 340/470 loss: 14.6503, time left (estimated): 203.27s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 350/470 loss: 14.6079, time left (estimated): 189.28s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 360/470 loss: 14.4857, time left (estimated): 173.19s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 370/470 loss: 14.3649, time left (estimated): 157.79s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 380/470 loss: 14.2443, time left (estimated): 141.99s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 390/470 loss: 14.1052, time left (estimated): 125.92s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 400/470 loss: 14.0019, time left (estimated): 110.22s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 410/470 loss: 13.8874, time left (estimated): 94.44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 420/470 loss: 13.7840, time left (estimated): 78.71s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 430/470 loss: 13.6245, time left (estimated): 62.55s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 440/470 loss: 13.5149, time left (estimated): 46.96s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 450/470 loss: 13.3729, time left (estimated): 31.20s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 460/470 loss: 13.2416, time left (estimated): 15.52s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain: 470/470 loss: 13.0803, time left (estimated): 0.00s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2021-01-20 10:52:28,835 - Nested Mention - INFO - train: 470 loss: 13.0803, time: 724.47s\n","2021-01-20 10:52:50,552 - Nested Mention - INFO - dev precision: 59.03%, recall: 35.51%, F1: 44.35%\n","2021-01-20 10:52:50,554 - Nested Mention - INFO - dev step took 21.7090 seconds\n","2021-01-20 10:52:50,556 - Nested Mention - INFO - \n","2021-01-20 10:52:51,291 - Nested Mention - INFO - training step took 746.9341 seconds\n","2021-01-20 10:52:51,293 - Nested Mention - INFO - best dev F1: 44.35%\n","2021-01-20 10:52:51,295 - Nested Mention - INFO - \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["2021-01-20 10:53:16,864 - Nested Mention - INFO - test precision: 0.00%, recall: 100.00%, F1: 0.00%\n","2021-01-20 10:53:16,867 - Nested Mention - INFO - test step took 25.5701 seconds\n","2021-01-20 10:53:16,869 - Nested Mention - INFO - Dumping model to ./dumps/NLPHW_model_210120_105251.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jCwsYSZKnUuS","executionInfo":{"status":"ok","timestamp":1611143366026,"user_tz":-480,"elapsed":966,"user":{"displayName":"hao ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOspQRqQvSKFKaSjazc9OUXKV7SHzuh9ZbzbYv=s64","userId":"06578137870287261288"}}},"source":["with open(this_model_path + '.result.txt','r') as f:\r\n","  lines = f.readlines()\r\n","with open('181220010.txt','w') as f:\r\n","  for i in range(len(lines)):\r\n","    if i%2!=0:\r\n","      f.write(lines[i])"],"execution_count":7,"outputs":[]}]}