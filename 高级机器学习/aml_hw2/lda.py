# -*- coding: utf-8 -*-
"""new_LDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1egtEhE5ahip1Z5uJCLANqt3-jppWMUl_
"""

# 在colab中，切换到工作目录
import os
if "news.txt" not in os.listdir():
    os.chdir('drive/MyDrive/LDA')
assert "news.txt" in os.listdir()

import numpy as np
import time

np.random.seed(2020) # 固定随机数种子，保证复现结果

marks = ",.!~`+-_=—“”‘’()[]\n"
stop_words = ["i","the",'in',"to","of","and","or","for","on","that","he",
"she","it","is","was","were","his","mr","with","you","from","a","an","no","not",
"at","but","as","are","be","has","have","we","who","they","by","had","would",
"its","their","which","this","said","about","my","been","her","after","one",
"will","there","ms","when","what","after","new","more","if","also","than",
"him","them","so","me","some","other","all","can","could"]

# 数据载入，预处理
def initialize_dataset(filename):
  word_ids = dict()
  ids_word = dict()
  docs = []
  current_doc = []
  current_word = 0
  for line in open(filename,'r',encoding='utf-8').readlines():
    # 统一小写
    new_line = line.lower()
    # 去除标点
    for mk in marks:
      new_line = new_line.replace(mk,' ')
    # 去除stop_words
    new_line = new_line.split()
    words = [w for w in new_line if w not in stop_words and len(w)>1]
    for w in words:
      if w in word_ids:
        current_doc.append(word_ids[w])
      else:
        current_doc.append(current_word)
        word_ids[w] = current_word
        ids_word[current_word] = w
        current_word = current_word + 1
    
    docs.append(current_doc);
    current_doc = []
  return docs, word_ids, ids_word

def initialize_gibbs(docs):
	# 进行一轮初始吉布斯采样
	for d, doc in enumerate(docs):
		tw = []
		for word in doc:
			p_t = np.divide(np.multiply(ntd[:,d], nwt[word,:]), nt)
			t = np.random.multinomial(1, p_t / p_t.sum()).argmax()
			tw.append(t)
			ntd[t][d] = ntd[t][d] + 1
			nwt[word,t] = nwt[word,t] + 1
			nt[t] = nt[t] + 1
		twd.append(np.array(tw))

def gibbs_iteration(docs):
	# 对每篇文章的每个词进行一轮采样
	for d, doc in enumerate(docs):
		for w, word in enumerate(doc):
			# 排除所选词的旧topic
			t = twd[d][w]
			ntd[t][d] = ntd[t][d] - 1
			nwt[word,t] = nwt[word,t] - 1
			nt[t] = nt[t] - 1
			# 采样新topic
			p_t = np.divide(np.multiply(ntd[:,d], nwt[word,:]), nt)
			t = np.random.multinomial(1, p_t / p_t.sum()).argmax()
			# 根据新topic增加相应计数值
			twd[d][w] = t 
			ntd[t][d] = ntd[t][d] + 1
			nwt[word,t] = nwt[word,t] + 1
			nt[t] = nt[t] + 1

def top_words(topic,num):
  ids_top_words = np.argsort(-nwt[:,topic])
  top_words = []
  top_freq = []
  for j in ids_top_words:
    top_words.append(ids_word[j])
    top_freq.append(nwt[:,topic][j])
  top_freq /= np.sum(top_freq)
  return top_words[:num],top_freq[:num]


docs, word_ids, ids_word = initialize_dataset("news.txt")
print("词袋大小：",len(word_ids))

alpha = 5 # 超参数
beta = 0.1 # 超参数
n_iter = 10 # 吉布斯采样循环次数，一轮约需要1-5分钟

for k in [5,10,20]:
  print("=====主题数：%d====="%k)
  twd = [] # topic of word w in doc d
  ntd = np.zeros((k, len(docs))) + alpha # number of words of topic t in doc d
  nwt = np.zeros((len(word_ids), k)) + beta # number of times word w is in topic t
  nt = np.zeros(k) + (len(word_ids) * beta) # number of words in topic t

  initialize_gibbs(docs)
  print(time.strftime('%X'), "Initialization Complete")

  for i in range(n_iter):
    gibbs_iteration(docs)
    print(time.strftime('%X'), "Gibbs_Iteration: ", i, " Completed")

  for t in range(k):
    words,freqs = top_words(t,10) # t类的前10高频词
    wf_dict = [(w,f) for w,f in zip(words,freqs)]
    print("主题%d:"%(t),wf_dict)