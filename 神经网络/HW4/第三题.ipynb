{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "第三题.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP8lvqjhLjst"
      },
      "source": [
        "# 切换工作目录，主要适用于google colab中的开发\n",
        "import os\n",
        "\n",
        "if \"第三题.ipynb\" not in os.listdir():\n",
        "  try:\n",
        "    os.chdir(\"drive/MyDrive\")\n",
        "    os.chdir(\"NN_Homework\")\n",
        "    os.chdir(\"HW4\")\n",
        "  except:\n",
        "    print(\"无法找到正确工作目录！\")\n",
        "\n",
        "assert \"第三题.ipynb\" in os.listdir()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZtMhbcTL79v",
        "outputId": "055c2abe-32db-485d-8b76-f56b0a069dd3"
      },
      "source": [
        "# 生成数据集\r\n",
        "import numpy as np\r\n",
        "import torch.utils.data as Data\r\n",
        "\r\n",
        "x_train = np.linspace(-np.pi, np.pi, 100).reshape(-1,1)\r\n",
        "y_train = np.sin(x_train)\r\n",
        "\r\n",
        "print(x_train.shape,y_train.shape)\r\n",
        "\r\n",
        "# 生成pytorch数据集\r\n",
        "y_train = torch.from_numpy(y_train).float()\r\n",
        "x_train = torch.from_numpy(x_train).float()\r\n",
        "train_set = Data.TensorDataset(x_train, y_train)\r\n",
        "train_iter = Data.DataLoader(train_set, batch_size=20, shuffle=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 1) (100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHr8m13pUOA_",
        "outputId": "54f09b13-616d-49d0-d87a-f2d06cf49b4f"
      },
      "source": [
        "import torch\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 设定设备\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "# 固定随机种子\r\n",
        "def set_random_seed(seed):\r\n",
        "    np.random.seed(seed)\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    torch.cuda.manual_seed(seed)\r\n",
        "    torch.backends.cudnn.deterministic = True\r\n",
        "    torch.backends.cudnn.benchmark = False\r\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\r\n",
        "set_random_seed(2020)\r\n",
        "\r\n",
        "\r\n",
        "class MLP(nn.Module):\r\n",
        "  def __init__(self, in_dim, hidden_dim):\r\n",
        "    super(MLP, self).__init__()\r\n",
        "    self.linear1 = nn.Linear(in_dim, hidden_dim)\r\n",
        "    self.linear2 = nn.Linear(hidden_dim, 1)\r\n",
        "  \r\n",
        "  def forward(self, x):\r\n",
        "    x = self.linear1(x)\r\n",
        "    x = F.relu(x)\r\n",
        "    x = self.linear2(x)\r\n",
        "    return x\r\n",
        "  \r\n",
        "  def out_by_layer(self, x):\r\n",
        "    x = self.linear1(x)\r\n",
        "    x = F.relu(x)\r\n",
        "    print(x)\r\n",
        "    x = self.linear2(x)\r\n",
        "    print(x)\r\n",
        "\r\n",
        "\r\n",
        "#instantiate model\r\n",
        "mlp = MLP(1, 5).to(device)\r\n",
        "mse = nn.MSELoss()\r\n",
        "optimizer = torch.optim.Adam(mlp.parameters())\r\n",
        "\r\n",
        "for epoch in range(100):\r\n",
        "  train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\r\n",
        "  for x, y in train_iter:\r\n",
        "    y_pred = mlp(x)\r\n",
        "    loss = mse(y_pred,y)\r\n",
        "\r\n",
        "    optimizer.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    train_l_sum += loss.item()\r\n",
        "    n += y.shape[0]\r\n",
        "  print('epoch %d, loss %.4f'%(epoch + 1, train_l_sum / n))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 0.0457\n",
            "epoch 2, loss 0.0438\n",
            "epoch 3, loss 0.0423\n",
            "epoch 4, loss 0.0407\n",
            "epoch 5, loss 0.0391\n",
            "epoch 6, loss 0.0377\n",
            "epoch 7, loss 0.0364\n",
            "epoch 8, loss 0.0352\n",
            "epoch 9, loss 0.0340\n",
            "epoch 10, loss 0.0328\n",
            "epoch 11, loss 0.0316\n",
            "epoch 12, loss 0.0306\n",
            "epoch 13, loss 0.0296\n",
            "epoch 14, loss 0.0287\n",
            "epoch 15, loss 0.0277\n",
            "epoch 16, loss 0.0268\n",
            "epoch 17, loss 0.0259\n",
            "epoch 18, loss 0.0251\n",
            "epoch 19, loss 0.0243\n",
            "epoch 20, loss 0.0236\n",
            "epoch 21, loss 0.0229\n",
            "epoch 22, loss 0.0222\n",
            "epoch 23, loss 0.0216\n",
            "epoch 24, loss 0.0209\n",
            "epoch 25, loss 0.0203\n",
            "epoch 26, loss 0.0197\n",
            "epoch 27, loss 0.0191\n",
            "epoch 28, loss 0.0186\n",
            "epoch 29, loss 0.0181\n",
            "epoch 30, loss 0.0176\n",
            "epoch 31, loss 0.0171\n",
            "epoch 32, loss 0.0166\n",
            "epoch 33, loss 0.0162\n",
            "epoch 34, loss 0.0157\n",
            "epoch 35, loss 0.0153\n",
            "epoch 36, loss 0.0149\n",
            "epoch 37, loss 0.0146\n",
            "epoch 38, loss 0.0142\n",
            "epoch 39, loss 0.0139\n",
            "epoch 40, loss 0.0135\n",
            "epoch 41, loss 0.0132\n",
            "epoch 42, loss 0.0129\n",
            "epoch 43, loss 0.0126\n",
            "epoch 44, loss 0.0124\n",
            "epoch 45, loss 0.0121\n",
            "epoch 46, loss 0.0119\n",
            "epoch 47, loss 0.0117\n",
            "epoch 48, loss 0.0115\n",
            "epoch 49, loss 0.0113\n",
            "epoch 50, loss 0.0111\n",
            "epoch 51, loss 0.0109\n",
            "epoch 52, loss 0.0107\n",
            "epoch 53, loss 0.0106\n",
            "epoch 54, loss 0.0104\n",
            "epoch 55, loss 0.0103\n",
            "epoch 56, loss 0.0102\n",
            "epoch 57, loss 0.0101\n",
            "epoch 58, loss 0.0099\n",
            "epoch 59, loss 0.0099\n",
            "epoch 60, loss 0.0097\n",
            "epoch 61, loss 0.0097\n",
            "epoch 62, loss 0.0096\n",
            "epoch 63, loss 0.0095\n",
            "epoch 64, loss 0.0094\n",
            "epoch 65, loss 0.0094\n",
            "epoch 66, loss 0.0093\n",
            "epoch 67, loss 0.0093\n",
            "epoch 68, loss 0.0092\n",
            "epoch 69, loss 0.0092\n",
            "epoch 70, loss 0.0091\n",
            "epoch 71, loss 0.0091\n",
            "epoch 72, loss 0.0091\n",
            "epoch 73, loss 0.0090\n",
            "epoch 74, loss 0.0090\n",
            "epoch 75, loss 0.0090\n",
            "epoch 76, loss 0.0090\n",
            "epoch 77, loss 0.0089\n",
            "epoch 78, loss 0.0089\n",
            "epoch 79, loss 0.0089\n",
            "epoch 80, loss 0.0089\n",
            "epoch 81, loss 0.0089\n",
            "epoch 82, loss 0.0088\n",
            "epoch 83, loss 0.0088\n",
            "epoch 84, loss 0.0088\n",
            "epoch 85, loss 0.0088\n",
            "epoch 86, loss 0.0088\n",
            "epoch 87, loss 0.0088\n",
            "epoch 88, loss 0.0088\n",
            "epoch 89, loss 0.0087\n",
            "epoch 90, loss 0.0087\n",
            "epoch 91, loss 0.0087\n",
            "epoch 92, loss 0.0087\n",
            "epoch 93, loss 0.0087\n",
            "epoch 94, loss 0.0087\n",
            "epoch 95, loss 0.0087\n",
            "epoch 96, loss 0.0087\n",
            "epoch 97, loss 0.0087\n",
            "epoch 98, loss 0.0087\n",
            "epoch 99, loss 0.0087\n",
            "epoch 100, loss 0.0086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64E3pBDvozn-",
        "outputId": "899757e1-b55c-4c93-d02f-00b3b650b4a4"
      },
      "source": [
        "for name, param in mlp.named_parameters():\r\n",
        "  print(name,param)\r\n",
        "mlp.out_by_layer(x_train)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "linear1.weight Parameter containing:\n",
            "tensor([[-0.0263],\n",
            "        [-0.6343],\n",
            "        [ 0.4072],\n",
            "        [-1.1352],\n",
            "        [ 0.1665]], requires_grad=True)\n",
            "linear1.bias Parameter containing:\n",
            "tensor([-0.4353, -0.0133,  0.4695,  0.0069,  0.1663], requires_grad=True)\n",
            "linear2.weight Parameter containing:\n",
            "tensor([[ 0.0360,  0.0840,  0.4442, -0.2487,  0.4166]], requires_grad=True)\n",
            "linear2.bias Parameter containing:\n",
            "tensor([-0.1576], requires_grad=True)\n",
            "tensor([[0.0000e+00, 1.9793e+00, 0.0000e+00, 3.5733e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.9391e+00, 0.0000e+00, 3.5013e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.8988e+00, 0.0000e+00, 3.4292e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.8585e+00, 0.0000e+00, 3.3572e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.8183e+00, 0.0000e+00, 3.2851e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.7780e+00, 0.0000e+00, 3.2131e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.7378e+00, 0.0000e+00, 3.1410e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.6975e+00, 0.0000e+00, 3.0690e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.6573e+00, 0.0000e+00, 2.9969e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.6170e+00, 0.0000e+00, 2.9249e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.5768e+00, 0.0000e+00, 2.8528e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.5365e+00, 0.0000e+00, 2.7808e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.4963e+00, 0.0000e+00, 2.7087e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.4560e+00, 0.0000e+00, 2.6367e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.4158e+00, 0.0000e+00, 2.5646e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.3755e+00, 0.0000e+00, 2.4926e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.3352e+00, 0.0000e+00, 2.4205e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.2950e+00, 0.0000e+00, 2.3485e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.2547e+00, 0.0000e+00, 2.2764e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.2145e+00, 0.0000e+00, 2.2044e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.1742e+00, 0.0000e+00, 2.1323e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.1340e+00, 0.0000e+00, 2.0603e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.0937e+00, 0.0000e+00, 1.9882e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.0535e+00, 0.0000e+00, 1.9162e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.0132e+00, 0.0000e+00, 1.8441e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 9.7296e-01, 0.0000e+00, 1.7721e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 9.3271e-01, 0.0000e+00, 1.7000e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 8.9245e-01, 0.0000e+00, 1.6280e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 8.5220e-01, 0.0000e+00, 1.5559e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 8.1194e-01, 0.0000e+00, 1.4839e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 7.7169e-01, 0.0000e+00, 1.4118e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 7.3144e-01, 0.0000e+00, 1.3398e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 6.9118e-01, 1.7208e-02, 1.2677e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 6.5093e-01, 4.3053e-02, 1.1957e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 6.1067e-01, 6.8898e-02, 1.1237e+00, 2.5408e-03],\n",
            "        [0.0000e+00, 5.7042e-01, 9.4743e-02, 1.0516e+00, 1.3106e-02],\n",
            "        [0.0000e+00, 5.3017e-01, 1.2059e-01, 9.7955e-01, 2.3671e-02],\n",
            "        [0.0000e+00, 4.8991e-01, 1.4643e-01, 9.0751e-01, 3.4236e-02],\n",
            "        [0.0000e+00, 4.4966e-01, 1.7228e-01, 8.3546e-01, 4.4802e-02],\n",
            "        [0.0000e+00, 4.0940e-01, 1.9812e-01, 7.6341e-01, 5.5367e-02],\n",
            "        [0.0000e+00, 3.6915e-01, 2.2397e-01, 6.9136e-01, 6.5932e-02],\n",
            "        [0.0000e+00, 3.2890e-01, 2.4981e-01, 6.1931e-01, 7.6497e-02],\n",
            "        [0.0000e+00, 2.8864e-01, 2.7566e-01, 5.4726e-01, 8.7063e-02],\n",
            "        [0.0000e+00, 2.4839e-01, 3.0150e-01, 4.7521e-01, 9.7628e-02],\n",
            "        [0.0000e+00, 2.0813e-01, 3.2735e-01, 4.0317e-01, 1.0819e-01],\n",
            "        [0.0000e+00, 1.6788e-01, 3.5319e-01, 3.3112e-01, 1.1876e-01],\n",
            "        [0.0000e+00, 1.2763e-01, 3.7904e-01, 2.5907e-01, 1.2932e-01],\n",
            "        [0.0000e+00, 8.7372e-02, 4.0488e-01, 1.8702e-01, 1.3989e-01],\n",
            "        [0.0000e+00, 4.7118e-02, 4.3073e-01, 1.1497e-01, 1.5045e-01],\n",
            "        [0.0000e+00, 6.8638e-03, 4.5657e-01, 4.2923e-02, 1.6102e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 4.8242e-01, 0.0000e+00, 1.7158e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 5.0826e-01, 0.0000e+00, 1.8215e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 5.3411e-01, 0.0000e+00, 1.9271e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 5.5995e-01, 0.0000e+00, 2.0328e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 5.8580e-01, 0.0000e+00, 2.1385e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 6.1165e-01, 0.0000e+00, 2.2441e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 6.3749e-01, 0.0000e+00, 2.3498e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 6.6334e-01, 0.0000e+00, 2.4554e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 6.8918e-01, 0.0000e+00, 2.5611e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 7.1503e-01, 0.0000e+00, 2.6667e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 7.4087e-01, 0.0000e+00, 2.7724e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 7.6672e-01, 0.0000e+00, 2.8780e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 7.9256e-01, 0.0000e+00, 2.9837e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 8.1841e-01, 0.0000e+00, 3.0893e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 8.4425e-01, 0.0000e+00, 3.1950e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 8.7010e-01, 0.0000e+00, 3.3006e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 8.9594e-01, 0.0000e+00, 3.4063e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 9.2179e-01, 0.0000e+00, 3.5119e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 9.4763e-01, 0.0000e+00, 3.6176e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 9.7348e-01, 0.0000e+00, 3.7232e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 9.9932e-01, 0.0000e+00, 3.8289e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.0252e+00, 0.0000e+00, 3.9345e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.0510e+00, 0.0000e+00, 4.0402e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.0769e+00, 0.0000e+00, 4.1458e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.1027e+00, 0.0000e+00, 4.2515e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.1285e+00, 0.0000e+00, 4.3572e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.1544e+00, 0.0000e+00, 4.4628e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.1802e+00, 0.0000e+00, 4.5685e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.2061e+00, 0.0000e+00, 4.6741e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.2319e+00, 0.0000e+00, 4.7798e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.2578e+00, 0.0000e+00, 4.8854e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.2836e+00, 0.0000e+00, 4.9911e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.3095e+00, 0.0000e+00, 5.0967e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.3353e+00, 0.0000e+00, 5.2024e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.3612e+00, 0.0000e+00, 5.3080e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.3870e+00, 0.0000e+00, 5.4137e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.4128e+00, 0.0000e+00, 5.5193e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.4387e+00, 0.0000e+00, 5.6250e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.4645e+00, 0.0000e+00, 5.7306e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.4904e+00, 0.0000e+00, 5.8363e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.5162e+00, 0.0000e+00, 5.9419e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.5421e+00, 0.0000e+00, 6.0476e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.5679e+00, 0.0000e+00, 6.1532e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.5938e+00, 0.0000e+00, 6.2589e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.6196e+00, 0.0000e+00, 6.3645e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.6454e+00, 0.0000e+00, 6.4702e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.6713e+00, 0.0000e+00, 6.5758e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.6971e+00, 0.0000e+00, 6.6815e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.7230e+00, 0.0000e+00, 6.7872e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 1.7488e+00, 0.0000e+00, 6.8928e-01]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "tensor([[-0.8799],\n",
            "        [-0.8653],\n",
            "        [-0.8508],\n",
            "        [-0.8363],\n",
            "        [-0.8217],\n",
            "        [-0.8072],\n",
            "        [-0.7927],\n",
            "        [-0.7781],\n",
            "        [-0.7636],\n",
            "        [-0.7491],\n",
            "        [-0.7345],\n",
            "        [-0.7200],\n",
            "        [-0.7055],\n",
            "        [-0.6909],\n",
            "        [-0.6764],\n",
            "        [-0.6619],\n",
            "        [-0.6473],\n",
            "        [-0.6328],\n",
            "        [-0.6183],\n",
            "        [-0.6037],\n",
            "        [-0.5892],\n",
            "        [-0.5747],\n",
            "        [-0.5601],\n",
            "        [-0.5456],\n",
            "        [-0.5311],\n",
            "        [-0.5165],\n",
            "        [-0.5020],\n",
            "        [-0.4875],\n",
            "        [-0.4729],\n",
            "        [-0.4584],\n",
            "        [-0.4439],\n",
            "        [-0.4293],\n",
            "        [-0.4072],\n",
            "        [-0.3811],\n",
            "        [-0.3541],\n",
            "        [-0.3237],\n",
            "        [-0.2932],\n",
            "        [-0.2628],\n",
            "        [-0.2324],\n",
            "        [-0.2020],\n",
            "        [-0.1716],\n",
            "        [-0.1412],\n",
            "        [-0.1108],\n",
            "        [-0.0803],\n",
            "        [-0.0499],\n",
            "        [-0.0195],\n",
            "        [ 0.0109],\n",
            "        [ 0.0413],\n",
            "        [ 0.0717],\n",
            "        [ 0.1021],\n",
            "        [ 0.1281],\n",
            "        [ 0.1440],\n",
            "        [ 0.1599],\n",
            "        [ 0.1758],\n",
            "        [ 0.1916],\n",
            "        [ 0.2075],\n",
            "        [ 0.2234],\n",
            "        [ 0.2393],\n",
            "        [ 0.2552],\n",
            "        [ 0.2710],\n",
            "        [ 0.2869],\n",
            "        [ 0.3028],\n",
            "        [ 0.3187],\n",
            "        [ 0.3346],\n",
            "        [ 0.3504],\n",
            "        [ 0.3663],\n",
            "        [ 0.3822],\n",
            "        [ 0.3981],\n",
            "        [ 0.4140],\n",
            "        [ 0.4299],\n",
            "        [ 0.4457],\n",
            "        [ 0.4616],\n",
            "        [ 0.4775],\n",
            "        [ 0.4934],\n",
            "        [ 0.5093],\n",
            "        [ 0.5251],\n",
            "        [ 0.5410],\n",
            "        [ 0.5569],\n",
            "        [ 0.5728],\n",
            "        [ 0.5887],\n",
            "        [ 0.6045],\n",
            "        [ 0.6204],\n",
            "        [ 0.6363],\n",
            "        [ 0.6522],\n",
            "        [ 0.6681],\n",
            "        [ 0.6839],\n",
            "        [ 0.6998],\n",
            "        [ 0.7157],\n",
            "        [ 0.7316],\n",
            "        [ 0.7475],\n",
            "        [ 0.7634],\n",
            "        [ 0.7792],\n",
            "        [ 0.7951],\n",
            "        [ 0.8110],\n",
            "        [ 0.8269],\n",
            "        [ 0.8428],\n",
            "        [ 0.8586],\n",
            "        [ 0.8745],\n",
            "        [ 0.8904],\n",
            "        [ 0.9063]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}