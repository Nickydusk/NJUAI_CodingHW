{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "#from dask_ml.metrics import mean_squared_error\n",
    "#from dask_ml.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error,explained_variance_score\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=8)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_data = dd.read_csv(\"/home/liweikang/Documents/hape/data.csv\",header=None)\n",
    "test_data = dd.read_csv(\"/run/media/liweikang/OS/Users/Li Weikang/Desktop/A_testData0531.csv\")\n",
    "e = time.time()\n",
    "train_data.columns = ['loadingOrder','carrierName','timestamp','longitude',\n",
    "                  'latitude','vesselMMSI','speed','direction','vesselNextport',\n",
    "                  'vesselNextportETA','vesselStatus','vesselDatasource','TRANSPORT_TRACE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_data(data, mode='train'):\n",
    "    \n",
    "    assert mode=='train' or mode=='test'\n",
    "    \n",
    "    if mode=='train':\n",
    "        data['vesselNextportETA'] = dd.to_datetime(data['vesselNextportETA'], infer_datetime_format=True)\n",
    "    elif mode=='test':\n",
    "        data['temp_timestamp'] = data['timestamp']\n",
    "        data['onboardDate'] = dd.to_datetime(data['onboardDate'], infer_datetime_format=True)\n",
    "    data['timestamp'] = dd.to_datetime(data['timestamp'], infer_datetime_format=True)\n",
    "    data['longitude'] = data['longitude'].astype(float)\n",
    "    data['loadingOrder'] = data['loadingOrder'].astype(str)\n",
    "    data['latitude'] = data['latitude'].astype(float)\n",
    "    data['speed'] = data['speed'].astype(float)\n",
    "    data['direction'] = data['direction'].astype(float)\n",
    "\n",
    "    return data\n",
    "\n",
    "s = time.time()\n",
    "train_data = get_data(train_data, mode='train')#.compute()\n",
    "test_data = get_data(test_data, mode='test')#.compute()\n",
    "e = time.time()\n",
    "print(\"Time = {}\".format(e-s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码参考：https://github.com/juzstu/TianChi_HaiYang\n",
    "def get_feature(df, mode='train'):\n",
    "    \n",
    "    assert mode=='train' or mode=='test'\n",
    "    ts = time.time()    \n",
    "    df = df.compute()\n",
    "    te = time.time()\n",
    "    print(\"Time = {}\".format(te-ts))\n",
    "    \n",
    "    t1 = time.time()\n",
    "    df.sort_values(['loadingOrder', 'timestamp'], inplace=True)\n",
    "    t2 = time.time()\n",
    "    print(\"Time = {}\".format(t2-t1))\n",
    "    # 特征只选择经纬度、速度\\方向\n",
    "    \n",
    "    df['lat_diff'] = df.groupby('loadingOrder')['latitude'].diff(1)\n",
    "    df['lon_diff'] = df.groupby('loadingOrder')['longitude'].diff(1)\n",
    "    df['speed_diff'] = df.groupby('loadingOrder')['speed'].diff(1)\n",
    "    df['diff_minutes'] = df.groupby('loadingOrder')['timestamp'].diff(1).dt.total_seconds() // 60\n",
    "    t = time.time()\n",
    "    \n",
    "    df = dd.from_pandas(df, npartitions=12)\n",
    "    \n",
    "    print(\"Time = {}\".format(t-t2))\n",
    "    df['anchor'] = df.apply(lambda x: 1 if x['lat_diff'] <= 0.03 and x['lon_diff'] <= 0.03\n",
    "                            and x['speed_diff'] <= 0.3 and x['diff_minutes'] <= 10 else 0, axis=1)\n",
    "    df = df.compute()\n",
    "    t3 = time.time()\n",
    "    print(\"Time = {}\".format(t3-t))\n",
    "    \n",
    "    if mode=='train':\n",
    "        group_df = df.groupby('loadingOrder')['timestamp'].agg(mmax='max', count='count', mmin='min').reset_index()\n",
    "        # 读取数据的最大值-最小值，即确认时间间隔为label\n",
    "        group_df['label'] = (group_df['mmax'] - group_df['mmin']).dt.total_seconds()\n",
    "    elif mode=='test':\n",
    "        group_df = df.groupby('loadingOrder')['timestamp'].agg(count='count').reset_index()\n",
    "    t4 = time.time()\n",
    "    print(\"Time = {}\".format(t4-t3))\n",
    "    \n",
    "    anchor_df = df.groupby('loadingOrder')['anchor'].agg('sum').reset_index()\n",
    "    anchor_df.columns = ['loadingOrder', 'anchor_cnt']\n",
    "    group_df = group_df.merge(anchor_df, on='loadingOrder', how='left')\n",
    "    group_df['anchor_ratio'] = group_df['anchor_cnt'] / group_df['count']\n",
    "\n",
    "    agg_function = ['min', 'max', 'mean', 'median']\n",
    "    agg_col = ['latitude', 'longitude', 'speed', 'direction']\n",
    "\n",
    "    group = df.groupby('loadingOrder')[agg_col].agg(agg_function).reset_index()\n",
    "    group.columns = ['loadingOrder'] + ['{}_{}'.format(i, j) for i in agg_col for j in agg_function]\n",
    "    group_df = group_df.merge(group, on='loadingOrder', how='left')\n",
    "    t5 = time.time()\n",
    "    print(\"Time = {}\".format(t5-t4))\n",
    "    \n",
    "    return group_df\n",
    "\n",
    "train = get_feature(train_data, mode='train')\n",
    "test = get_feature(test_data, mode='test')\n",
    "features = [c for c in train.columns if c not in ['loadingOrder', 'label', 'mmin', 'mmax', 'count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_score_eval(preds, valid):\n",
    "    labels = valid.get_label()\n",
    "    scores = mean_squared_error(y_true=labels, y_pred=preds)\n",
    "    return 'mse_score', scores, True\n",
    "\n",
    "def build_model(train, test, pred, label, seed=1080, is_shuffle=True):\n",
    "    train_pred = np.zeros((train.shape[0], ))\n",
    "    test_pred = np.zeros((test.shape[0], ))\n",
    "    n_splits = 10\n",
    "    # Kfold\n",
    "    fold = KFold(n_splits=n_splits, shuffle=is_shuffle, random_state=seed)\n",
    "    kf_way = fold.split(train[pred])\n",
    "    # params\n",
    "    params = {\n",
    "        'learning_rate': 0.01,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'num_leaves': 36,\n",
    "        'feature_fraction': 0.6,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 6,\n",
    "        'seed': 8,\n",
    "        'bagging_seed': 1,\n",
    "        'feature_fraction_seed': 7,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'nthread': 8,\n",
    "        'verbose': 1,\n",
    "    }\n",
    "    # train\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(kf_way, start=1):\n",
    "        train_x, train_y = train[pred].iloc[train_idx], train[label].iloc[train_idx]\n",
    "        valid_x, valid_y = train[pred].iloc[valid_idx], train[label].iloc[valid_idx]\n",
    "        # 数据加载\n",
    "        n_train = lgb.Dataset(train_x, label=train_y)\n",
    "        n_valid = lgb.Dataset(valid_x, label=valid_y)\n",
    "\n",
    "        clf = lgb.train(\n",
    "            params=params,\n",
    "            train_set=n_train,\n",
    "            num_boost_round=3000,\n",
    "            valid_sets=[n_valid],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=100,\n",
    "            feval=mse_score_eval\n",
    "        )\n",
    "        train_pred[valid_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration)\n",
    "        test_pred += clf.predict(test[pred], num_iteration=clf.best_iteration)/fold.n_splits\n",
    "    \n",
    "    test['label'] = test_pred\n",
    "    \n",
    "    return test[['loadingOrder', 'label']]\n",
    "\n",
    "result = build_model(train, test, features, 'label', is_shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.merge(result, on='loadingOrder', how='left')\n",
    "test_data['ETA'] = (test_data['onboardDate'] + test_data['label'].apply(lambda x:pd.Timedelta(seconds=x))).apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "# A = test_data['onboardDate']\n",
    "# B = test_data['label'].apply(lambda x:pd.Timedelta(seconds=x))\n",
    "# C = A+B\n",
    "# test_data['ETA'] = C.apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "test_data.drop(['direction','TRANSPORT_TRACE'],axis=1,inplace=True)\n",
    "test_data['onboardDate'] = test_data['onboardDate'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "test_data['creatDate'] = pd.datetime.now().strftime('%Y/%m/%d  %H:%M:%S')\n",
    "test_data['timestamp'] = test_data['temp_timestamp']\n",
    "# 整理columns顺序\n",
    "result = test_data[['loadingOrder', 'timestamp', 'longitude', 'latitude', 'carrierName', 'vesselMMSI', 'onboardDate', 'ETA', 'creatDate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
