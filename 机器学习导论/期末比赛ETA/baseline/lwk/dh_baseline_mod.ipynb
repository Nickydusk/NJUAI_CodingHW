{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:45087</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>16</li>\n",
       "  <li><b>Memory: </b>16.63 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:45087' processes=8 threads=16, memory=16.63 GB>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,explained_variance_score\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=8)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline只用到gps定位数据，即train_gps_path\n",
    "train_gps_path = '/run/media/liweikang/OS/Users/Li Weikang/Desktop/train0523.csv'\n",
    "test_data_path = 'test_wash_origin.csv'\n",
    "order_data_path = 'wash1_event.csv'\n",
    "port_data_path = 'port_fixed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取前1000000行\n",
    "debug = True\n",
    "NDATA = 1000000\n",
    "\n",
    "if debug:\n",
    "    train_data = pd.read_csv(train_gps_path,nrows=NDATA,header=None)\n",
    "else:\n",
    "    train_data = pd.read_csv(train_gps_path,header=None)\n",
    "\n",
    "train_data.columns = ['loadingOrder','carrierName','timestamp','longitude',\n",
    "                  'latitude','vesselMMSI','speed','direction','vesselNextport',\n",
    "                  'vesselNextportETA','vesselStatus','vesselDatasource','TRANSPORT_TRACE']\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "event_data = pd.read_csv(order_data_path)\n",
    "port = pd.read_csv(port_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data, mode='train'):\n",
    "    \n",
    "    assert mode=='train' or mode=='test'\n",
    "    \n",
    "    if mode=='train':\n",
    "        data['vesselNextportETA'] = pd.to_datetime(data['vesselNextportETA'], infer_datetime_format=True)\n",
    "    elif mode=='test':\n",
    "        data['temp_timestamp'] = data['timestamp']\n",
    "        data['onboardDate'] = pd.to_datetime(data['onboardDate'], infer_datetime_format=True)\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'], infer_datetime_format=True)\n",
    "    data['longitude'] = data['longitude'].astype(float)\n",
    "    data['loadingOrder'] = data['loadingOrder'].astype(str)\n",
    "    data['latitude'] = data['latitude'].astype(float)\n",
    "    data['speed'] = data['speed'].astype(float)\n",
    "    data['direction'] = data['direction'].astype(float)\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data = get_data(train_data, mode='train')\n",
    "test_data = get_data(test_data, mode='test')\n",
    "event_data['timestamp'] = pd.to_datetime(event_data['timestamp'], infer_datetime_format=True, utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码参考：https://github.com/juzstu/TianChi_HaiYang\n",
    "def get_feature(df, mode='train'):\n",
    "    \n",
    "    assert mode=='train' or mode=='test'\n",
    "    \n",
    "    df.sort_values(['loadingOrder', 'timestamp'], inplace=True)\n",
    "    # 特征只选择经纬度、速度\\方向\n",
    "    df['lat_diff'] = df.groupby('loadingOrder')['latitude'].diff(1)\n",
    "    df['lon_diff'] = df.groupby('loadingOrder')['longitude'].diff(1)\n",
    "    df['speed_diff'] = df.groupby('loadingOrder')['speed'].diff(1)\n",
    "    df['diff_minutes'] = df.groupby('loadingOrder')['timestamp'].diff(1).dt.total_seconds() // 60\n",
    "    \n",
    "    df = dd.from_pandas(df, npartitions=16)\n",
    "    df['anchor'] = df.apply(lambda x: 1 if x['lat_diff'] <= 0.03 and x['lon_diff'] <= 0.03\n",
    "                            and x['speed_diff'] <= 0.3 and x['diff_minutes'] <= 10 else 0, axis=1)\n",
    "    df = df.compute()\n",
    "    \n",
    "    #print(\"df:\",df)\n",
    "    if mode=='train':\n",
    "        group_df = df.groupby('loadingOrder')['timestamp'].agg(mmax='max', count='count', mmin='min').reset_index()\n",
    "        # 读取数据的最大值-最小值，即确认时间间隔为label\n",
    "        # group_df['label'] = (group_df['mmax'] - group_df['mmin']).dt.total_seconds()\n",
    "        # print(group_df)\n",
    "        \n",
    "        event_df = event_data[event_data['loadingOrder'].isin(group_df['loadingOrder'].values)][['loadingOrder','latitude','longitude','timestamp']]\n",
    "        #print(\"event: \",event_df)\n",
    "\n",
    "        tmp = pd.concat([df[['loadingOrder','latitude','longitude','timestamp']], event_df], axis=0, ignore_index=True)\n",
    "        #print(\"tmp:\",tmp)\n",
    "        \n",
    "        tmp['timestamp'] = pd.to_datetime(tmp['timestamp'], infer_datetime_format=True)\n",
    "        tmp.sort_values(['loadingOrder', 'timestamp'], inplace=True)\n",
    "        \n",
    "        tmp_time = tmp.groupby('loadingOrder')['timestamp'].agg(mmax='max', count='count', mmin='min').reset_index()\n",
    "        tmp_time['label'] = (tmp_time['mmax'] - tmp_time['mmin']).dt.total_seconds()\n",
    "        tmp_time = tmp_time[['loadingOrder','label']]\n",
    "        group_df = group_df.merge(tmp_time,on='loadingOrder',how='left')\n",
    "        # print(group_df)\n",
    "        \n",
    "        tmp_cord = tmp.groupby('loadingOrder')['latitude','longitude'].last().reset_index()\n",
    "        tmp_cord2 = df.groupby('loadingOrder')['latitude','longitude'].last().reset_index()\n",
    "        \n",
    "        def func(order,lat,lon,tmp2):\n",
    "            if lat==200:\n",
    "                r = tmp2[tmp2['loadingOrder']==order][['latitude','longitude']].values\n",
    "                #print(\"r is:\", r)\n",
    "                return r[0][0],r[0][1]\n",
    "            else:\n",
    "                return lat,lon\n",
    "        \n",
    "        tmp_cord = dd.from_pandas(tmp_cord, npartitions=16)\n",
    "        tmp_cord[['latitude','longitude']] = tmp_cord.apply(lambda x: func(x['loadingOrder'],x['latitude'],x['longitude'],tmp_cord2),axis=1,result_type='expand')\n",
    "        tmp_cord = tmp_cord.compute()\n",
    "        \n",
    "        tmp_cord.columns = ['loadingOrder','end_lat','end_lon']\n",
    "        group_df = group_df.merge(tmp_cord,on='loadingOrder',how='left')\n",
    "        \n",
    "        \n",
    "    elif mode=='test':\n",
    "        group_df = df.groupby('loadingOrder')['timestamp'].agg(count='count').reset_index()\n",
    "        \n",
    "        tmp_cordinate = df.groupby('loadingOrder')['end_lat','end_lon'].last().reset_index()\n",
    "        group_df = group_df.merge(tmp_cordinate, on='loadingOrder', how='left')\n",
    "        \n",
    "\n",
    "    anchor_df = df.groupby('loadingOrder')['anchor'].agg('sum').reset_index()\n",
    "    anchor_df.columns = ['loadingOrder', 'anchor_cnt']\n",
    "    group_df = group_df.merge(anchor_df, on='loadingOrder', how='left')\n",
    "    group_df['anchor_ratio'] = group_df['anchor_cnt'] / group_df['count']\n",
    "\n",
    "    agg_function = ['min', 'max', 'mean', 'median']\n",
    "    agg_col = ['latitude', 'longitude', 'speed', 'direction']\n",
    "\n",
    "    group = df.groupby('loadingOrder')[agg_col].agg(agg_function).reset_index()\n",
    "    group.columns = ['loadingOrder'] + ['{}_{}'.format(i, j) for i in agg_col for j in agg_function]\n",
    "    group_df = group_df.merge(group, on='loadingOrder', how='left')\n",
    "    \n",
    "    return group_df\n",
    "\n",
    "\n",
    "train = get_feature(train_data, mode='train')\n",
    "#print(train)\n",
    "test = get_feature(test_data, mode='test')\n",
    "#print(test)\n",
    "features = [c for c in train.columns if c not in ['loadingOrder', 'label', 'mmin', 'mmax', 'count']]\n",
    "#print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l2: 1.21705e+13\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's l2: 1.15576e+13\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l2: 2.03019e+12\n",
      "[200]\tvalid_0's l2: 1.95045e+12\n",
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's l2: 1.9113e+12\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l2: 1.7417e+12\n",
      "[200]\tvalid_0's l2: 1.75041e+12\n",
      "Early stopping, best iteration is:\n",
      "[126]\tvalid_0's l2: 1.70693e+12\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l2: 1.0231e+13\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid_0's l2: 1.01387e+13\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l2: 1.79867e+12\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid_0's l2: 1.67417e+12\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l2: 1.81215e+12\n",
      "[200]\tvalid_0's l2: 1.73296e+12\n",
      "Early stopping, best iteration is:\n",
      "[194]\tvalid_0's l2: 1.68574e+12\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l2: 2.32752e+12\n",
      "Early stopping, best iteration is:\n",
      "[73]\tvalid_0's l2: 2.25906e+12\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l2: 1.78185e+12\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's l2: 1.60275e+12\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l2: 4.18027e+12\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid_0's l2: 4.10878e+12\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l2: 1.93695e+12\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid_0's l2: 1.8912e+12\n"
     ]
    }
   ],
   "source": [
    "def build_model(train, test, pred, label, seed=1080, is_shuffle=True):\n",
    "    train_pred = np.zeros((train.shape[0], ))\n",
    "    test_pred = np.zeros((test.shape[0], ))\n",
    "    n_splits = 10\n",
    "    # Kfold\n",
    "    fold = KFold(n_splits=n_splits, shuffle=is_shuffle, random_state=seed)\n",
    "    kf_way = fold.split(train[pred])\n",
    "    # params\n",
    "    params = {\n",
    "        'learning_rate': 0.05,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'num_leaves': 36,\n",
    "        'metric': 'mse',\n",
    "        'feature_fraction': 0.48,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 6,\n",
    "        'seed': 8,\n",
    "        'bagging_seed': 1,\n",
    "        'feature_fraction_seed': 7,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'nthread': 8,\n",
    "        'verbose': 1,\n",
    "    }\n",
    "    # train\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(kf_way, start=1):\n",
    "        train_x, train_y = train[pred].iloc[train_idx], train[label].iloc[train_idx]\n",
    "        valid_x, valid_y = train[pred].iloc[valid_idx], train[label].iloc[valid_idx]\n",
    "        # 数据加载\n",
    "        n_train = lgb.Dataset(train_x, label=train_y)\n",
    "        n_valid = lgb.Dataset(valid_x, label=valid_y)\n",
    "\n",
    "        clf = lgb.train(\n",
    "            params=params,\n",
    "            train_set=n_train,\n",
    "            num_boost_round=3000,\n",
    "            valid_sets=[n_valid],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=100\n",
    "        )\n",
    "        train_pred[valid_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration)\n",
    "        test_pred += clf.predict(test[pred], num_iteration=clf.best_iteration)/fold.n_splits\n",
    "    \n",
    "    test['label'] = test_pred\n",
    "    \n",
    "    return test[['loadingOrder', 'label']]\n",
    "\n",
    "result = build_model(train, test, features, 'label', is_shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.merge(result, on='loadingOrder', how='left')\n",
    "test_data['ETA'] = (test_data['onboardDate'] + test_data['label'].apply(lambda x:pd.Timedelta(seconds=x))).apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "test_data.drop(['direction','TRANSPORT_TRACE'],axis=1,inplace=True)\n",
    "test_data['onboardDate'] = test_data['onboardDate'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "test_data['creatDate'] = pd.datetime.now().strftime('%Y/%m/%d  %H:%M:%S')\n",
    "test_data['timestamp'] = test_data['temp_timestamp']\n",
    "# 整理columns顺序\n",
    "result = test_data[['loadingOrder', 'timestamp', 'longitude', 'latitude', 'carrierName', 'vesselMMSI', 'onboardDate', 'ETA', 'creatDate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loadingOrder</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>carrierName</th>\n",
       "      <th>vesselMMSI</th>\n",
       "      <th>onboardDate</th>\n",
       "      <th>ETA</th>\n",
       "      <th>creatDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CF946210847851</td>\n",
       "      <td>2019-04-02T02:42:28.000Z</td>\n",
       "      <td>138.471062</td>\n",
       "      <td>40.278787</td>\n",
       "      <td>OIEQNT</td>\n",
       "      <td>R5480015614</td>\n",
       "      <td>2019/04/02  02:42:28</td>\n",
       "      <td>2019/04/17  20:19:39</td>\n",
       "      <td>2020/06/17  11:40:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CF946210847851</td>\n",
       "      <td>2019-04-02T02:59:28.000Z</td>\n",
       "      <td>138.552168</td>\n",
       "      <td>40.327785</td>\n",
       "      <td>OIEQNT</td>\n",
       "      <td>R5480015614</td>\n",
       "      <td>2019/04/02  02:42:28</td>\n",
       "      <td>2019/04/17  20:19:39</td>\n",
       "      <td>2020/06/17  11:40:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CF946210847851</td>\n",
       "      <td>2019-04-02T03:07:28.000Z</td>\n",
       "      <td>138.588250</td>\n",
       "      <td>40.352542</td>\n",
       "      <td>OIEQNT</td>\n",
       "      <td>R5480015614</td>\n",
       "      <td>2019/04/02  02:42:28</td>\n",
       "      <td>2019/04/17  20:19:39</td>\n",
       "      <td>2020/06/17  11:40:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CF946210847851</td>\n",
       "      <td>2019-04-02T03:43:28.000Z</td>\n",
       "      <td>138.751325</td>\n",
       "      <td>40.459447</td>\n",
       "      <td>OIEQNT</td>\n",
       "      <td>R5480015614</td>\n",
       "      <td>2019/04/02  02:42:28</td>\n",
       "      <td>2019/04/17  20:19:39</td>\n",
       "      <td>2020/06/17  11:40:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CF946210847851</td>\n",
       "      <td>2019-04-02T04:29:28.000Z</td>\n",
       "      <td>138.969782</td>\n",
       "      <td>40.581485</td>\n",
       "      <td>OIEQNT</td>\n",
       "      <td>R5480015614</td>\n",
       "      <td>2019/04/02  02:42:28</td>\n",
       "      <td>2019/04/17  20:19:39</td>\n",
       "      <td>2020/06/17  11:40:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45451</th>\n",
       "      <td>XG479584941731</td>\n",
       "      <td>2019-01-13T03:56:08.000Z</td>\n",
       "      <td>104.633357</td>\n",
       "      <td>1.630708</td>\n",
       "      <td>JCMFTA</td>\n",
       "      <td>U2218600548</td>\n",
       "      <td>2019/01/10  00:27:58</td>\n",
       "      <td>2019/02/14  06:59:15</td>\n",
       "      <td>2020/06/17  11:40:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45452</th>\n",
       "      <td>XG479584941731</td>\n",
       "      <td>2019-01-13T03:57:08.000Z</td>\n",
       "      <td>104.631958</td>\n",
       "      <td>1.626713</td>\n",
       "      <td>JCMFTA</td>\n",
       "      <td>U2218600548</td>\n",
       "      <td>2019/01/10  00:27:58</td>\n",
       "      <td>2019/02/14  06:59:15</td>\n",
       "      <td>2020/06/17  11:40:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45453</th>\n",
       "      <td>XG479584941731</td>\n",
       "      <td>2019-01-13T03:57:38.000Z</td>\n",
       "      <td>104.631258</td>\n",
       "      <td>1.624615</td>\n",
       "      <td>JCMFTA</td>\n",
       "      <td>U2218600548</td>\n",
       "      <td>2019/01/10  00:27:58</td>\n",
       "      <td>2019/02/14  06:59:15</td>\n",
       "      <td>2020/06/17  11:40:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45454</th>\n",
       "      <td>XG479584941731</td>\n",
       "      <td>2019-01-13T03:58:08.000Z</td>\n",
       "      <td>104.630597</td>\n",
       "      <td>1.622682</td>\n",
       "      <td>JCMFTA</td>\n",
       "      <td>U2218600548</td>\n",
       "      <td>2019/01/10  00:27:58</td>\n",
       "      <td>2019/02/14  06:59:15</td>\n",
       "      <td>2020/06/17  11:40:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45455</th>\n",
       "      <td>XG479584941731</td>\n",
       "      <td>2019-01-13T03:59:08.000Z</td>\n",
       "      <td>104.629178</td>\n",
       "      <td>1.618552</td>\n",
       "      <td>JCMFTA</td>\n",
       "      <td>U2218600548</td>\n",
       "      <td>2019/01/10  00:27:58</td>\n",
       "      <td>2019/02/14  06:59:15</td>\n",
       "      <td>2020/06/17  11:40:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45456 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         loadingOrder                 timestamp   longitude   latitude  \\\n",
       "0      CF946210847851  2019-04-02T02:42:28.000Z  138.471062  40.278787   \n",
       "1      CF946210847851  2019-04-02T02:59:28.000Z  138.552168  40.327785   \n",
       "2      CF946210847851  2019-04-02T03:07:28.000Z  138.588250  40.352542   \n",
       "3      CF946210847851  2019-04-02T03:43:28.000Z  138.751325  40.459447   \n",
       "4      CF946210847851  2019-04-02T04:29:28.000Z  138.969782  40.581485   \n",
       "...               ...                       ...         ...        ...   \n",
       "45451  XG479584941731  2019-01-13T03:56:08.000Z  104.633357   1.630708   \n",
       "45452  XG479584941731  2019-01-13T03:57:08.000Z  104.631958   1.626713   \n",
       "45453  XG479584941731  2019-01-13T03:57:38.000Z  104.631258   1.624615   \n",
       "45454  XG479584941731  2019-01-13T03:58:08.000Z  104.630597   1.622682   \n",
       "45455  XG479584941731  2019-01-13T03:59:08.000Z  104.629178   1.618552   \n",
       "\n",
       "      carrierName   vesselMMSI           onboardDate                   ETA  \\\n",
       "0          OIEQNT  R5480015614  2019/04/02  02:42:28  2019/04/17  20:19:39   \n",
       "1          OIEQNT  R5480015614  2019/04/02  02:42:28  2019/04/17  20:19:39   \n",
       "2          OIEQNT  R5480015614  2019/04/02  02:42:28  2019/04/17  20:19:39   \n",
       "3          OIEQNT  R5480015614  2019/04/02  02:42:28  2019/04/17  20:19:39   \n",
       "4          OIEQNT  R5480015614  2019/04/02  02:42:28  2019/04/17  20:19:39   \n",
       "...           ...          ...                   ...                   ...   \n",
       "45451      JCMFTA  U2218600548  2019/01/10  00:27:58  2019/02/14  06:59:15   \n",
       "45452      JCMFTA  U2218600548  2019/01/10  00:27:58  2019/02/14  06:59:15   \n",
       "45453      JCMFTA  U2218600548  2019/01/10  00:27:58  2019/02/14  06:59:15   \n",
       "45454      JCMFTA  U2218600548  2019/01/10  00:27:58  2019/02/14  06:59:15   \n",
       "45455      JCMFTA  U2218600548  2019/01/10  00:27:58  2019/02/14  06:59:15   \n",
       "\n",
       "                  creatDate  \n",
       "0      2020/06/17  11:40:35  \n",
       "1      2020/06/17  11:40:35  \n",
       "2      2020/06/17  11:40:35  \n",
       "3      2020/06/17  11:40:35  \n",
       "4      2020/06/17  11:40:35  \n",
       "...                     ...  \n",
       "45451  2020/06/17  11:40:35  \n",
       "45452  2020/06/17  11:40:35  \n",
       "45453  2020/06/17  11:40:35  \n",
       "45454  2020/06/17  11:40:35  \n",
       "45455  2020/06/17  11:40:35  \n",
       "\n",
       "[45456 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result_m.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}